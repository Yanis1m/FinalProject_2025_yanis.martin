{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a68ed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded data with 7176 users and 10728 items\n",
      "Sparsity: 0.866194\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, ndcg_score, mean_absolute_error\n",
    "\n",
    "# For Two-Tower model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Layer, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('outputs/models', exist_ok=True)\n",
    "os.makedirs('outputs/figures', exist_ok=True)\n",
    "os.makedirs('outputs/recommendations', exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_interactions = pd.read_csv('outputs/data/processed/train_interactions.csv')\n",
    "test_interactions = pd.read_csv('outputs/data/processed/test_interactions.csv')\n",
    "interaction_agg = pd.read_csv('outputs/data/processed/interaction_agg.csv')\n",
    "\n",
    "# Load sparse matrices\n",
    "train_sparse = sparse.load_npz('outputs/data/processed/train_sparse.npz')\n",
    "test_sparse = sparse.load_npz('outputs/data/processed/test_sparse.npz')\n",
    "\n",
    "# Load mappings\n",
    "with open('outputs/data/processed/user_mapping.pkl', 'rb') as f:\n",
    "    user_id_to_idx, idx_to_user_id = pickle.load(f)\n",
    "    \n",
    "with open('outputs/data/processed/video_mapping.pkl', 'rb') as f:\n",
    "    video_id_to_idx, idx_to_video_id = pickle.load(f)\n",
    "\n",
    "# Load neural network features\n",
    "user_nn_features = pd.read_csv('outputs/data/processed/user_nn_features.csv')\n",
    "video_nn_features = pd.read_csv('outputs/data/processed/video_nn_features.csv')\n",
    "\n",
    "# Load scalers and PCA objects\n",
    "with open('outputs/data/processed/target_scaler.pkl', 'rb') as f:\n",
    "    target_scaler = pickle.load(f)\n",
    "\n",
    "n_users = train_sparse.shape[0]\n",
    "n_items = train_sparse.shape[1]\n",
    "\n",
    "print(f\"Loaded data with {n_users} users and {n_items} items\")\n",
    "print(f\"Sparsity: {1.0 - (train_sparse.count_nonzero() / (n_users * n_items)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1548f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom L2 normalization layer for Two-Tower model\n",
    "class L2NormalizationLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.l2_normalize(inputs, axis=1)\n",
    "\n",
    "# Define Two-Tower model architecture\n",
    "def create_two_tower_model(user_dim, item_dim, embedding_dim=64):\n",
    "    # User tower\n",
    "    user_input = Input(shape=(user_dim,), name='user_input')\n",
    "    user_tower = Sequential([\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(embedding_dim, activation='relu', name='user_embedding'),\n",
    "        L2NormalizationLayer()\n",
    "    ], name='user_tower')\n",
    "    user_embedding = user_tower(user_input)\n",
    "    \n",
    "    # Item tower\n",
    "    item_input = Input(shape=(item_dim,), name='item_input')\n",
    "    item_tower = Sequential([\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(embedding_dim, activation='relu', name='item_embedding'),\n",
    "        L2NormalizationLayer()\n",
    "    ], name='item_tower')\n",
    "    item_embedding = item_tower(item_input)\n",
    "    \n",
    "    # Combine towers\n",
    "    merged = Concatenate()([user_embedding, item_embedding])\n",
    "    x = Dense(128, activation='relu')(merged)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output, name='TwoTowerRecommender')\n",
    "    \n",
    "    return model, user_tower, item_tower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b57f8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing reduced feature files...\n"
     ]
    }
   ],
   "source": [
    "# Check if reduced feature files exist, if not, create them\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Check if the files exist\n",
    "if (not os.path.exists('outputs/data/processed/user_features_train_reduced.npy') or\n",
    "    not os.path.exists('outputs/data/processed/item_features_train_reduced.npy') or\n",
    "    not os.path.exists('outputs/data/processed/user_features_test_reduced.npy') or\n",
    "    not os.path.exists('outputs/data/processed/item_features_test_reduced.npy')):\n",
    "    \n",
    "    print(\"Reduced feature files not found. Generating them now...\")\n",
    "    \n",
    "    # Load necessary data\n",
    "    train_interactions = pd.read_csv('outputs/data/processed/train_interactions.csv')\n",
    "    test_interactions = pd.read_csv('outputs/data/processed/test_interactions.csv')\n",
    "    user_features = pd.read_csv('outputs/data/processed/user_features_processed.csv')\n",
    "    item_features = pd.read_csv('outputs/data/processed/item_features_processed.csv')\n",
    "    \n",
    "    # Create mapping dictionaries for faster lookups\n",
    "    user_features_dict = user_features.set_index('user_id').to_dict('index')\n",
    "    item_features_dict = item_features.set_index('video_id').to_dict('index')\n",
    "    \n",
    "    # Extract user features\n",
    "    user_feature_cols = [col for col in user_features.columns if col != 'user_id']\n",
    "    user_features_train = np.array([\n",
    "        [user_features_dict.get(uid, {}).get(col, 0) for col in user_feature_cols]\n",
    "        for uid in train_interactions['user_id']\n",
    "    ])\n",
    "    user_features_test = np.array([\n",
    "        [user_features_dict.get(uid, {}).get(col, 0) for col in user_feature_cols]\n",
    "        for uid in test_interactions['user_id']\n",
    "    ])\n",
    "    \n",
    "    # Extract item features\n",
    "    item_feature_cols = [col for col in item_features.columns if col != 'video_id']\n",
    "    item_features_train = np.array([\n",
    "        [item_features_dict.get(vid, {}).get(col, 0) for col in item_feature_cols]\n",
    "        for vid in train_interactions['video_id']\n",
    "    ])\n",
    "    item_features_test = np.array([\n",
    "        [item_features_dict.get(vid, {}).get(col, 0) for col in item_feature_cols]\n",
    "        for vid in test_interactions['video_id']\n",
    "    ])\n",
    "    \n",
    "    # Scale features\n",
    "    user_scaler = StandardScaler()\n",
    "    item_scaler = StandardScaler()\n",
    "    \n",
    "    user_features_train_scaled = user_scaler.fit_transform(user_features_train)\n",
    "    item_features_train_scaled = item_scaler.fit_transform(item_features_train)\n",
    "    user_features_test_scaled = user_scaler.transform(user_features_test)\n",
    "    item_features_test_scaled = item_scaler.transform(item_features_test)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_user = PCA(n_components=0.95)\n",
    "    pca_item = PCA(n_components=0.95)\n",
    "    \n",
    "    user_features_train_reduced = pca_user.fit_transform(user_features_train_scaled)\n",
    "    item_features_train_reduced = pca_item.fit_transform(item_features_train_scaled)\n",
    "    user_features_test_reduced = pca_user.transform(user_features_test_scaled)\n",
    "    item_features_test_reduced = pca_item.transform(item_features_test_scaled)\n",
    "    \n",
    "    # Save the reduced features\n",
    "    np.save('outputs/data/processed/user_features_train_reduced.npy', user_features_train_reduced)\n",
    "    np.save('outputs/data/processed/item_features_train_reduced.npy', item_features_train_reduced)\n",
    "    np.save('outputs/data/processed/user_features_test_reduced.npy', user_features_test_reduced)\n",
    "    np.save('outputs/data/processed/item_features_test_reduced.npy', item_features_test_reduced)\n",
    "    \n",
    "    # Save scalers and PCA objects\n",
    "    with open('outputs/data/processed/user_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(user_scaler, f)\n",
    "    with open('outputs/data/processed/item_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(item_scaler, f)\n",
    "    with open('outputs/data/processed/pca_user.pkl', 'wb') as f:\n",
    "        pickle.dump(pca_user, f)\n",
    "    with open('outputs/data/processed/pca_item.pkl', 'wb') as f:\n",
    "        pickle.dump(pca_item, f)\n",
    "    \n",
    "    print(f\"Created reduced features: User shape={user_features_train_reduced.shape}, Item shape={item_features_train_reduced.shape}\")\n",
    "else:\n",
    "    print(\"Loading existing reduced feature files...\")\n",
    "\n",
    "# Now load the files (they should exist now)\n",
    "user_features_train_reduced = np.load('outputs/data/processed/user_features_train_reduced.npy')\n",
    "item_features_train_reduced = np.load('outputs/data/processed/item_features_train_reduced.npy')\n",
    "user_features_test_reduced = np.load('outputs/data/processed/user_features_test_reduced.npy')\n",
    "item_features_test_reduced = np.load('outputs/data/processed/item_features_test_reduced.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a52fc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Two-Tower model...\n",
      "User features shape: (2000000, 23)\n",
      "Item features shape: (2000000, 51)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Two-Tower model\n",
    "print(\"Preparing data for Two-Tower model...\")\n",
    "\n",
    "# Get user and item dimensions\n",
    "user_dim = len([col for col in user_nn_features.columns if col.startswith('pca_user_')])\n",
    "item_dim = len([col for col in video_nn_features.columns if col.startswith('pca_item_')])\n",
    "\n",
    "# Create user and item feature matrices\n",
    "user_features_matrix = user_nn_features[[col for col in user_nn_features.columns if col.startswith('pca_user_')]].values\n",
    "item_features_matrix = video_nn_features[[col for col in video_nn_features.columns if col.startswith('pca_item_')]].values\n",
    "\n",
    "# Extract target values\n",
    "y_train = train_interactions[['watch_ratio']].values\n",
    "y_test = test_interactions[['watch_ratio']].values\n",
    "y_train_scaled = target_scaler.transform(y_train)\n",
    "y_test_scaled = target_scaler.transform(y_test)\n",
    "\n",
    "print(f\"User features shape: {user_features_train_reduced.shape}\")\n",
    "print(f\"Item features shape: {item_features_train_reduced.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a076606b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training Two-Tower model...\n",
      "Epoch 1/30\n",
      "\u001b[1m3906/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9741 - mae: 0.3520\n",
      "Epoch 1: val_loss improved from inf to 0.58865, saving model to outputs/models/two_tower_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 5ms/step - loss: 0.9741 - mae: 0.3520 - val_loss: 0.5887 - val_mae: 0.2414\n",
      "Epoch 2/30\n",
      "\u001b[1m3896/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9890 - mae: 0.3362\n",
      "Epoch 2: val_loss improved from 0.58865 to 0.58492, saving model to outputs/models/two_tower_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 0.9889 - mae: 0.3362 - val_loss: 0.5849 - val_mae: 0.2427\n",
      "Epoch 3/30\n",
      "\u001b[1m3903/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9742 - mae: 0.3319\n",
      "Epoch 3: val_loss did not improve from 0.58492\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 0.9742 - mae: 0.3319 - val_loss: 0.5849 - val_mae: 0.2482\n",
      "Epoch 4/30\n",
      "\u001b[1m3905/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9553 - mae: 0.3294\n",
      "Epoch 4: val_loss did not improve from 0.58492\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 0.9553 - mae: 0.3294 - val_loss: 0.5911 - val_mae: 0.2652\n",
      "Epoch 5/30\n",
      "\u001b[1m3896/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9431 - mae: 0.3285\n",
      "Epoch 5: val_loss did not improve from 0.58492\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 0.9432 - mae: 0.3285 - val_loss: 0.5896 - val_mae: 0.2650\n",
      "Epoch 6/30\n",
      "\u001b[1m3903/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9124 - mae: 0.3268\n",
      "Epoch 6: val_loss did not improve from 0.58492\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 0.9125 - mae: 0.3268 - val_loss: 0.5942 - val_mae: 0.2800\n",
      "Epoch 7/30\n",
      "\u001b[1m3896/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9471 - mae: 0.3270\n",
      "Epoch 7: val_loss did not improve from 0.58492\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 0.9471 - mae: 0.3270 - val_loss: 0.5894 - val_mae: 0.2664\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train Two-Tower model\n",
    "print(\"Building and training Two-Tower model...\")\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 30\n",
    "\n",
    "# Build model\n",
    "two_tower_model, user_tower, item_tower = create_two_tower_model(user_dim, item_dim, embedding_dim)\n",
    "two_tower_model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'outputs/models/two_tower_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = two_tower_model.fit(\n",
    "    [user_features_train_reduced, item_features_train_reduced],\n",
    "    y_train_scaled,\n",
    "    validation_data=([user_features_test_reduced, item_features_test_reduced], y_test_scaled),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/two_tower_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Save the models\n",
    "user_tower.save('outputs/models/user_tower.h5')\n",
    "item_tower.save('outputs/models/item_tower.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46ca3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 872us/step\n",
      "Baseline MAE: 0.4866\n",
      "Model MAE: 0.4078\n",
      "Improvement: 16.19%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with Two-Tower model\n",
    "print(\"Generating predictions...\")\n",
    "test_predictions = two_tower_model.predict([user_features_test_reduced, item_features_test_reduced])\n",
    "y_pred = target_scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'user_id': test_interactions['user_id'],\n",
    "    'video_id': test_interactions['video_id'],\n",
    "    'true_watch_ratio': test_interactions['watch_ratio'],\n",
    "    'predicted_watch_ratio': y_pred.flatten()\n",
    "})\n",
    "\n",
    "# Calculate error metrics\n",
    "results_df['absolute_error'] = abs(results_df['predicted_watch_ratio'] - results_df['true_watch_ratio'])\n",
    "\n",
    "# Calculate baseline (mean) prediction\n",
    "mean_watch_ratio = results_df['true_watch_ratio'].mean()\n",
    "baseline_mae = mean_absolute_error(results_df['true_watch_ratio'], np.full_like(results_df['true_watch_ratio'], mean_watch_ratio))\n",
    "model_mae = results_df['absolute_error'].mean()\n",
    "\n",
    "print(f\"Baseline MAE: {baseline_mae:.4f}\")\n",
    "print(f\"Model MAE: {model_mae:.4f}\")\n",
    "print(f\"Improvement: {(baseline_mae - model_mae) / baseline_mae:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5289311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@25: 0.9624\n",
      "Precision@25: 0.8783\n",
      "Recall@25: 0.0335\n",
      "Evaluation complete. Results saved to outputs/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Define relevance threshold and top-k\n",
    "RELEVANCE_THRESHOLD = 0.8\n",
    "K = 25\n",
    "\n",
    "# Mark relevant items\n",
    "results_df['relevant'] = (results_df['true_watch_ratio'] >= RELEVANCE_THRESHOLD).astype(int)\n",
    "\n",
    "# Group by user and rank predictions\n",
    "results_df['rank'] = results_df.groupby('user_id')['predicted_watch_ratio'].rank(method='dense', ascending=False)\n",
    "\n",
    "# Filter to top-K recommendations per user\n",
    "top_k_recommendations = results_df[results_df['rank'] <= K]\n",
    "\n",
    "# Calculate metrics properly\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "ndcg_values = []\n",
    "\n",
    "for user_id in top_k_recommendations['user_id'].unique():\n",
    "    # Get all recommendations for this user\n",
    "    user_recs = top_k_recommendations[top_k_recommendations['user_id'] == user_id]\n",
    "    user_recs = user_recs.sort_values('rank')\n",
    "    \n",
    "    # Get all relevant items for this user (from the entire results_df, not just top-K)\n",
    "    all_user_items = results_df[results_df['user_id'] == user_id]\n",
    "    total_relevant_items = all_user_items['relevant'].sum()\n",
    "    \n",
    "    if total_relevant_items == 0:\n",
    "        continue  # Skip users with no relevant items\n",
    "    \n",
    "    # Calculate precision and recall for this user\n",
    "    relevant_in_topk = user_recs['relevant'].sum()\n",
    "    precision = relevant_in_topk / min(K, len(user_recs))\n",
    "    recall = relevant_in_topk / total_relevant_items\n",
    "    \n",
    "    precision_values.append(precision)\n",
    "    recall_values.append(recall)\n",
    "    \n",
    "    # Calculate NDCG for this user\n",
    "    if relevant_in_topk > 0:\n",
    "        # Create arrays for NDCG calculation without using pivot\n",
    "        # This avoids the duplicate index error\n",
    "        y_true = np.zeros(K)\n",
    "        y_score = np.zeros(K)\n",
    "        \n",
    "        # Fill arrays with values from user_recs\n",
    "        for i, (_, row) in enumerate(user_recs.iterrows()):\n",
    "            if i < K:\n",
    "                rank_idx = min(int(row['rank']) - 1, K-1)  # Convert rank to 0-based index\n",
    "                y_true[rank_idx] = row['relevant']\n",
    "                y_score[rank_idx] = row['predicted_watch_ratio']\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        user_ndcg = ndcg_score(np.array([y_true]), np.array([y_score]))\n",
    "        ndcg_values.append(user_ndcg)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_precision = np.mean(precision_values) if precision_values else 0\n",
    "avg_recall = np.mean(recall_values) if recall_values else 0\n",
    "avg_ndcg = np.mean(ndcg_values) if ndcg_values else 0\n",
    "\n",
    "print(f\"NDCG@{K}: {avg_ndcg:.4f}\")\n",
    "print(f\"Precision@{K}: {avg_precision:.4f}\")\n",
    "print(f\"Recall@{K}: {avg_recall:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    'MAE': model_mae,\n",
    "    'Baseline_MAE': baseline_mae,\n",
    "    f'NDCG@{K}': avg_ndcg,\n",
    "    f'Precision@{K}': avg_precision,\n",
    "    f'Recall@{K}': avg_recall\n",
    "}\n",
    "\n",
    "pd.DataFrame([eval_results]).to_csv('outputs/evaluation_results.csv', index=False)\n",
    "print(\"Evaluation complete. Results saved to outputs/evaluation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc88a2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final recommendations...\n",
      "Computing item embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Two-Tower recommendations: 100%|██████████| 100/100 [00:12<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 14 recommendations for 100 users\n",
      "Recommendation generation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate final recommendations\n",
    "print(\"Generating final recommendations...\")\n",
    "\n",
    "def generate_recommendations(num_users=100, k=50):\n",
    "    \"\"\"Generate recommendations for a subset of users using the Two-Tower model\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Select a subset of users\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    all_user_indices = list(idx_to_user_id.keys())\n",
    "    selected_users = np.random.choice(all_user_indices, min(num_users, len(all_user_indices)), replace=False)\n",
    "    \n",
    "    # Pre-compute all item embeddings once\n",
    "    print(\"Computing item embeddings...\")\n",
    "    item_embeddings = item_tower.predict(item_features_test_reduced, verbose=0)\n",
    "    \n",
    "    # Generate recommendations for selected users\n",
    "    for user_idx in tqdm(selected_users, desc=\"Generating Two-Tower recommendations\"):\n",
    "        if user_idx < len(user_features_test_reduced):\n",
    "            # Get user features\n",
    "            user_features = user_features_test_reduced[user_idx:user_idx+1]\n",
    "            \n",
    "            # Get user embedding\n",
    "            user_embedding = user_tower.predict(user_features, verbose=0)\n",
    "            \n",
    "            # Calculate scores using dot product\n",
    "            scores = np.dot(user_embedding, item_embeddings.T)[0]\n",
    "            \n",
    "            # Get top-k items\n",
    "            top_indices = np.argsort(-scores)[:k]\n",
    "            top_scores = scores[top_indices]\n",
    "            \n",
    "            for i, (item_idx, score) in enumerate(zip(top_indices, top_scores)):\n",
    "                if user_idx in idx_to_user_id and item_idx < len(idx_to_video_id):\n",
    "                    user_id = idx_to_user_id[user_idx]\n",
    "                    video_id = idx_to_video_id[item_idx]\n",
    "                    recommendations.append((user_id, video_id, float(score), i+1))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    recs_df = pd.DataFrame(recommendations, columns=['user_id', 'video_id', 'score', 'rank'])\n",
    "    return recs_df\n",
    "\n",
    "# Generate recommendations for 100 users\n",
    "two_tower_recommendations = generate_recommendations(num_users=100, k=50)\n",
    "two_tower_recommendations.to_csv('outputs/recommendations/two_tower_recommendations.csv', index=False)\n",
    "\n",
    "print(f\"Generated {len(two_tower_recommendations)} recommendations for 100 users\")\n",
    "print(\"Recommendation generation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f1352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Loaded 11564987 interactions from big matrix\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import os\n",
    "from scipy import sparse\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Load processed data\n",
    "print(\"Loading processed data...\")\n",
    "big_matrix = pd.read_csv('outputs/data/processed/big_matrix_processed.csv')\n",
    "small_matrix = pd.read_csv('outputs/data/processed/small_matrix_processed.csv')\n",
    "item_features_df = pd.read_csv('outputs/data/processed/item_features_processed.csv')\n",
    "user_features_selected = pd.read_csv('outputs/data/processed/user_features_processed.csv')\n",
    "\n",
    "print(f\"Loaded {len(big_matrix)} interactions from big matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0074b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user and video mappings...\n"
     ]
    }
   ],
   "source": [
    "# Create user and video mappings\n",
    "print(\"Creating user and video mappings...\")\n",
    "# Convert user_id and video_id to categorical indices\n",
    "big_matrix['user_id'] = big_matrix['user_id'].astype('category')\n",
    "big_matrix['video_id'] = big_matrix['video_id'].astype('category')\n",
    "\n",
    "# Create interaction matrix with user and video categorical codes\n",
    "interaction_matrix = big_matrix[['user_id', 'video_id', 'watch_ratio', 'play_duration', 'video_duration']].copy()\n",
    "interaction_matrix['user_idx'] = interaction_matrix['user_id'].cat.codes\n",
    "interaction_matrix['video_idx'] = interaction_matrix['video_id'].cat.codes\n",
    "\n",
    "# Create mapping dictionaries for later use\n",
    "user_id_to_idx = dict(zip(interaction_matrix['user_id'], interaction_matrix['user_idx']))\n",
    "video_id_to_idx = dict(zip(interaction_matrix['video_id'], interaction_matrix['video_idx']))\n",
    "idx_to_user_id = dict(zip(interaction_matrix['user_idx'], interaction_matrix['user_id']))\n",
    "idx_to_video_id = dict(zip(interaction_matrix['video_idx'], interaction_matrix['video_id']))\n",
    "\n",
    "# Save these mappings for later use\n",
    "with open('outputs/data/processed/user_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump((user_id_to_idx, idx_to_user_id), f)\n",
    "    \n",
    "with open('outputs/data/processed/video_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump((video_id_to_idx, idx_to_video_id), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8560fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user and video mappings...\n"
     ]
    }
   ],
   "source": [
    "# Create user and video mappings\n",
    "print(\"Creating user and video mappings...\")\n",
    "# Convert user_id and video_id to categorical indices\n",
    "big_matrix['user_id'] = big_matrix['user_id'].astype('category')\n",
    "big_matrix['video_id'] = big_matrix['video_id'].astype('category')\n",
    "\n",
    "# Create interaction matrix with user and video categorical codes\n",
    "interaction_matrix = big_matrix[['user_id', 'video_id', 'watch_ratio', 'play_duration', 'video_duration']].copy()\n",
    "interaction_matrix['user_idx'] = interaction_matrix['user_id'].cat.codes\n",
    "interaction_matrix['video_idx'] = interaction_matrix['video_id'].cat.codes\n",
    "\n",
    "# Create mapping dictionaries for later use\n",
    "user_id_to_idx = dict(zip(interaction_matrix['user_id'], interaction_matrix['user_idx']))\n",
    "video_id_to_idx = dict(zip(interaction_matrix['video_id'], interaction_matrix['video_idx']))\n",
    "idx_to_user_id = dict(zip(interaction_matrix['user_idx'], interaction_matrix['user_id']))\n",
    "idx_to_video_id = dict(zip(interaction_matrix['video_idx'], interaction_matrix['video_id']))\n",
    "\n",
    "# Save these mappings for later use\n",
    "with open('outputs/data/processed/user_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump((user_id_to_idx, idx_to_user_id), f)\n",
    "    \n",
    "with open('outputs/data/processed/video_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump((video_id_to_idx, idx_to_video_id), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e50b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced interaction features...\n"
     ]
    }
   ],
   "source": [
    "# Calculate engagement metrics per user-video pair\n",
    "print(\"Creating enhanced interaction features...\")\n",
    "interaction_agg = interaction_matrix.groupby(['user_idx', 'video_idx']).agg({\n",
    "    'watch_ratio': ['mean', 'sum', 'count'],\n",
    "    'play_duration': ['mean', 'sum'],\n",
    "    'video_duration': ['mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level column names\n",
    "interaction_agg.columns = ['user_idx', 'video_idx', 'watch_ratio_mean', 'watch_ratio_sum', \n",
    "                          'interaction_count', 'play_duration_mean', 'play_duration_sum', \n",
    "                          'video_duration_mean']\n",
    "\n",
    "# Calculate additional engagement metrics\n",
    "interaction_agg['completion_rate'] = np.minimum(interaction_agg['play_duration_mean'] / \n",
    "                                              interaction_agg['video_duration_mean'], 1.0)\n",
    "interaction_agg['replay_factor'] = interaction_agg['interaction_count'] / interaction_agg['interaction_count'].mean()\n",
    "\n",
    "# Cap extremely high watch_ratio values\n",
    "max_watch_ratio = 3.0\n",
    "interaction_agg['watch_ratio_capped'] = interaction_agg['watch_ratio_mean'].clip(upper=max_watch_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b86d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating advanced confidence scores...\n"
     ]
    }
   ],
   "source": [
    "# Calculate advanced confidence scores\n",
    "print(\"Calculating advanced confidence scores...\")\n",
    "\n",
    "# Calculate confidence using a more sophisticated approach for better recall\n",
    "base_confidence = 2.0  # Higher base value\n",
    "alpha = 60  # Much higher weight for watch_ratio\n",
    "beta = 0.8   # Higher weight for completion_rate\n",
    "gamma = 0.5  # Higher weight for replay_factor\n",
    "\n",
    "# First, create the recall-focused confidence\n",
    "interaction_agg['confidence_recall'] = base_confidence + \\\n",
    "                              alpha * np.log1p(interaction_agg['watch_ratio_capped']) * \\\n",
    "                              (1 + beta * interaction_agg['completion_rate']) * \\\n",
    "                              (1 + gamma * np.log1p(interaction_agg['replay_factor']))\n",
    "\n",
    "# Create a precision-focused confidence with more conservative parameters\n",
    "base_confidence_precision = 1.0\n",
    "alpha_precision = 40\n",
    "beta_precision = 0.5\n",
    "gamma_precision = 0.3\n",
    "\n",
    "interaction_agg['confidence_precision'] = base_confidence_precision + \\\n",
    "                              alpha_precision * np.log1p(interaction_agg['watch_ratio_capped']) * \\\n",
    "                              (1 + beta_precision * interaction_agg['completion_rate']) * \\\n",
    "                              (1 + gamma_precision * np.log1p(interaction_agg['replay_factor']))\n",
    "\n",
    "# Use the recall-focused confidence as the default\n",
    "interaction_agg['confidence'] = interaction_agg['confidence_recall']\n",
    "\n",
    "# Visualize confidence distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(interaction_agg['confidence'], bins=50)\n",
    "plt.title('Distribution of Confidence Scores')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('outputs/figures/confidence_distribution.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48255fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling interactions for training and testing...\n",
      "Training set: 2000000 interactions\n",
      "Testing set: 2000000 interactions\n"
     ]
    }
   ],
   "source": [
    "# Sample interactions for training and testing\n",
    "print(\"Sampling interactions for training and testing...\")\n",
    "SAMPLE_SIZE = 2_000_000  # Number of interactions to sample\n",
    "\n",
    "# Sample from big matrix for training\n",
    "train_interactions = big_matrix.sample(n=min(SAMPLE_SIZE, len(big_matrix)), random_state=42)\n",
    "\n",
    "# Get unique users and videos from training\n",
    "train_users = set(train_interactions['user_id'])\n",
    "train_videos = set(train_interactions['video_id'])\n",
    "\n",
    "# Filter small matrix for testing to ensure we have features for all users and videos\n",
    "test_interactions = small_matrix[\n",
    "    small_matrix['user_id'].isin(train_users) & \n",
    "    small_matrix['video_id'].isin(train_videos)\n",
    "].sample(n=min(SAMPLE_SIZE, len(small_matrix)), random_state=42)\n",
    "\n",
    "# Add user and video indices to test interactions\n",
    "test_interactions['user_idx'] = test_interactions['user_id'].map(user_id_to_idx)\n",
    "test_interactions['video_idx'] = test_interactions['video_id'].map(video_id_to_idx)\n",
    "\n",
    "print(f\"Training set: {len(train_interactions)} interactions\")\n",
    "print(f\"Testing set: {len(test_interactions)} interactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d15ff419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for users and items...\n",
      "User features shape: (2000000, 26)\n",
      "Item features shape: (2000000, 69)\n"
     ]
    }
   ],
   "source": [
    "# Extract user and item features\n",
    "print(\"Extracting features for users and items...\")\n",
    "\n",
    "# Create mapping dictionaries for faster lookups\n",
    "user_features_dict = user_features_selected.set_index('user_id').to_dict('index')\n",
    "item_features_dict = item_features_df.set_index('video_id').to_dict('index')\n",
    "\n",
    "# Extract user features\n",
    "user_feature_cols = [col for col in user_features_selected.columns if col != 'user_id']\n",
    "user_features_train = np.array([\n",
    "    [user_features_dict.get(uid, {}).get(col, 0) for col in user_feature_cols]\n",
    "    for uid in train_interactions['user_id']\n",
    "])\n",
    "user_features_test = np.array([\n",
    "    [user_features_dict.get(uid, {}).get(col, 0) for col in user_feature_cols]\n",
    "    for uid in test_interactions['user_id']\n",
    "])\n",
    "\n",
    "# Extract item features\n",
    "item_feature_cols = [col for col in item_features_df.columns if col != 'video_id']\n",
    "item_features_train = np.array([\n",
    "    [item_features_dict.get(vid, {}).get(col, 0) for col in item_feature_cols]\n",
    "    for vid in train_interactions['video_id']\n",
    "])\n",
    "item_features_test = np.array([\n",
    "    [item_features_dict.get(vid, {}).get(col, 0) for col in item_feature_cols]\n",
    "    for vid in test_interactions['video_id']\n",
    "])\n",
    "\n",
    "print(f\"User features shape: {user_features_train.shape}\")\n",
    "print(f\"Item features shape: {item_features_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12633e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling features and applying dimensionality reduction...\n",
      "Reduced user features shape: (2000000, 23)\n",
      "Reduced item features shape: (2000000, 51)\n",
      "Explained variance (user): 0.9639\n",
      "Explained variance (item): 0.9543\n"
     ]
    }
   ],
   "source": [
    "# Scale features and apply dimensionality reduction\n",
    "print(\"Scaling features and applying dimensionality reduction...\")\n",
    "\n",
    "# Scale item features\n",
    "item_scaler = StandardScaler()\n",
    "item_features_train_scaled = item_scaler.fit_transform(item_features_train)\n",
    "item_features_test_scaled = item_scaler.transform(item_features_test)\n",
    "\n",
    "# Scale user features\n",
    "user_scaler = StandardScaler()\n",
    "user_features_train_scaled = user_scaler.fit_transform(user_features_train)\n",
    "user_features_test_scaled = user_scaler.transform(user_features_test)\n",
    "\n",
    "# Scale target values\n",
    "target_scaler = StandardScaler()\n",
    "y_train = train_interactions[['watch_ratio']].values\n",
    "y_test = test_interactions[['watch_ratio']].values\n",
    "y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "y_test_scaled = target_scaler.transform(y_test)\n",
    "\n",
    "# Apply PCA to reduce dimensionality while preserving 95% variance\n",
    "pca_item = PCA(n_components=0.95)\n",
    "item_features_train_reduced = pca_item.fit_transform(item_features_train_scaled)\n",
    "item_features_test_reduced = pca_item.transform(item_features_test_scaled)\n",
    "\n",
    "pca_user = PCA(n_components=0.95)\n",
    "user_features_train_reduced = pca_user.fit_transform(user_features_train_scaled)\n",
    "user_features_test_reduced = pca_user.transform(user_features_test_scaled)\n",
    "\n",
    "print(f\"Reduced user features shape: {user_features_train_reduced.shape}\")\n",
    "print(f\"Reduced item features shape: {item_features_train_reduced.shape}\")\n",
    "print(f\"Explained variance (user): {sum(pca_user.explained_variance_ratio_):.4f}\")\n",
    "print(f\"Explained variance (item): {sum(pca_item.explained_variance_ratio_):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5235087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating neural network features...\n",
      "Creating user and video indices...\n",
      "Created sparse matrices with 7176 users and 10728 items\n",
      "Sparsity: 0.866194\n"
     ]
    }
   ],
   "source": [
    "# Create neural network features\n",
    "print(\"Creating neural network features...\")\n",
    "\n",
    "# Create user and video indices if they don't exist\n",
    "if 'user_idx' not in train_interactions.columns:\n",
    "    print(\"Creating user and video indices...\")\n",
    "    train_interactions['user_id'] = train_interactions['user_id'].astype('category')\n",
    "    train_interactions['video_id'] = train_interactions['video_id'].astype('category')\n",
    "    train_interactions['user_idx'] = train_interactions['user_id'].cat.codes\n",
    "    train_interactions['video_idx'] = train_interactions['video_id'].cat.codes\n",
    "\n",
    "# Create DataFrames with reduced features\n",
    "user_features_nn_df = pd.DataFrame(\n",
    "    user_features_train_reduced, \n",
    "    columns=[f'pca_user_{i}' for i in range(user_features_train_reduced.shape[1])]\n",
    ")\n",
    "user_features_nn_df['user_idx'] = train_interactions['user_idx'].values\n",
    "\n",
    "item_features_nn_df = pd.DataFrame(\n",
    "    item_features_train_reduced, \n",
    "    columns=[f'pca_item_{i}' for i in range(item_features_train_reduced.shape[1])]\n",
    ")\n",
    "item_features_nn_df['video_idx'] = train_interactions['video_idx'].values\n",
    "\n",
    "# Save neural network features\n",
    "user_features_nn_df.to_csv('outputs/data/processed/user_nn_features.csv', index=False)\n",
    "item_features_nn_df.to_csv('outputs/data/processed/video_nn_features.csv', index=False)\n",
    "\n",
    "# Create sparse matrices for ALS model\n",
    "n_users = max(interaction_agg['user_idx']) + 1\n",
    "n_items = max(interaction_agg['video_idx']) + 1\n",
    "\n",
    "# Create training matrix with confidence values\n",
    "train_sparse = sparse.csr_matrix(\n",
    "    (interaction_agg['confidence'], (interaction_agg['user_idx'], interaction_agg['video_idx'])),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "# Create test matrix with watch_ratio for evaluation\n",
    "test_sparse = sparse.csr_matrix(\n",
    "    (test_interactions['watch_ratio'], (test_interactions['user_idx'], test_interactions['video_idx'])),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "# Create binary matrix for BPR model\n",
    "train_binary = sparse.csr_matrix(\n",
    "    (np.ones(len(interaction_agg)), (interaction_agg['user_idx'], interaction_agg['video_idx'])),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "# Save sparse matrices\n",
    "sparse.save_npz('outputs/data/processed/train_sparse.npz', train_sparse)\n",
    "sparse.save_npz('outputs/data/processed/test_sparse.npz', test_sparse)\n",
    "sparse.save_npz('outputs/data/processed/train_binary.npz', train_binary)\n",
    "\n",
    "print(f\"Created sparse matrices with {n_users} users and {n_items} items\")\n",
    "print(f\"Sparsity: {1.0 - (len(interaction_agg) / float(n_users * n_items)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77cd3ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction sequences for sequential models...\n",
      "Average sequence length: 20.00\n",
      "Median sequence length: 20.00\n",
      "Max sequence length: 20\n",
      "Feature engineering complete!\n"
     ]
    }
   ],
   "source": [
    "# Create sequence features\n",
    "print(\"Creating interaction sequences for sequential models...\")\n",
    "\n",
    "# Sort interactions by timestamp for each user\n",
    "sorted_interactions = big_matrix.sort_values(['user_id', 'timestamp'])\n",
    "\n",
    "# Create sequences of video interactions per user\n",
    "max_seq_length = 20  # Maximum sequence length\n",
    "user_sequences = {}\n",
    "\n",
    "for user_id, group in sorted_interactions.groupby('user_id'):\n",
    "    if user_id in user_id_to_idx:\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        videos = group['video_id'].map(video_id_to_idx).dropna().astype(int).tolist()\n",
    "        \n",
    "        # Keep only the most recent interactions if sequence is too long\n",
    "        if len(videos) > max_seq_length:\n",
    "            videos = videos[-max_seq_length:]\n",
    "            \n",
    "        user_sequences[user_idx] = videos\n",
    "\n",
    "# Calculate sequence statistics\n",
    "seq_lengths = [len(seq) for seq in user_sequences.values()]\n",
    "avg_seq_length = np.mean(seq_lengths)\n",
    "median_seq_length = np.median(seq_lengths)\n",
    "max_seq_length = max(seq_lengths)\n",
    "\n",
    "print(f\"Average sequence length: {avg_seq_length:.2f}\")\n",
    "print(f\"Median sequence length: {median_seq_length:.2f}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "\n",
    "# Save sequences\n",
    "with open('outputs/data/processed/user_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(user_sequences, f)\n",
    "\n",
    "# Save train and test interactions\n",
    "train_interactions.to_csv('outputs/data/processed/train_interactions.csv', index=False)\n",
    "test_interactions.to_csv('outputs/data/processed/test_interactions.csv', index=False)\n",
    "\n",
    "# Save interaction aggregations\n",
    "interaction_agg.to_csv('outputs/data/processed/interaction_agg.csv', index=False)\n",
    "\n",
    "# Save scalers and PCA objects\n",
    "with open('outputs/data/processed/user_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(user_scaler, f)\n",
    "with open('outputs/data/processed/item_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(item_scaler, f)\n",
    "with open('outputs/data/processed/target_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(target_scaler, f)\n",
    "with open('outputs/data/processed/pca_user.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_user, f)\n",
    "with open('outputs/data/processed/pca_item.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_item, f)\n",
    "\n",
    "print(\"Feature engineering complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
